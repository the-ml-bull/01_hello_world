{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHw3AEd0T1zIs7NmOP5Ym5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/the-ml-bull/Hello_World/blob/main/5_Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yf2R03XvJagf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "from datetime import datetime \n",
        "\n",
        "def load_data():\n",
        "  url = 'https://raw.githubusercontent.com/the-ml-bull/Hello_World/main/Fx60.csv'\n",
        "  dateparse = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M')\n",
        "\n",
        "  df = pd.read_csv(url, parse_dates=['date'], date_parser=dateparse)\n",
        "\n",
        "  return df "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " def create_x_values(df, feature_names):\n",
        "\n",
        "  x_values_df = pd.DataFrame()\n",
        "\n",
        "  # loop thorugh feature name and \"back periods\" to go back \n",
        "  x_feature_names = []\n",
        "  for feature in feature_names:\n",
        "    for period in [1,2,3,4]:\n",
        "      # create the name (eg 'x_audusd_close_t-1')\n",
        "      feature_name = 'x_' + feature + '_t-' + str(period)\n",
        "      x_feature_names.append(feature_name)\n",
        "      x_values_df[feature_name] = df[feature].shift(period)\n",
        "\n",
        "  # Add \"starting\" values when used in normalization \n",
        "  x_values_df['x_audusd_open'] = df['audusd_open'].shift(4)\n",
        "  x_values_df['x_eurusd_open'] = df['eurusd_open'].shift(4)\n",
        "  x_values_df['audusd_open'] = df['audusd_open']\n",
        "  x_values_df['eurusd_open'] = df['eurusd_open']\n",
        "  \n",
        "  # add all future y values for future periods\n",
        "  for period in [0,1,2,3]:\n",
        "    name = 'y_t-' + str(period)\n",
        "    x_values_df[name] = df['audusd_close'].shift(-period)\n",
        "\n",
        "  # y is points 4 periods into the future - the open price now (not close)\n",
        "  x_values_df['y_future'] = df['audusd_close'].shift(-3)\n",
        "  x_values_df['y_change_price'] = x_values_df['y_future'] - df['audusd_open']\n",
        "  x_values_df['y_change_points'] = x_values_df['y_change_price'] * 100000 \n",
        "  x_values_df['y'] = np.where(x_values_df['y_change_points'] >= 200, 1, 0)\n",
        "\n",
        "  # and reset df and done \n",
        "  x_values_df = x_values_df.copy()\n",
        "  return x_values_df, x_feature_names"
      ],
      "metadata": {
        "id": "mmwuBM2zJikB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "def normalize_data(df, x_fields, method):\n",
        "  \n",
        "  norm_df = df.copy()\n",
        "  y_fields = ['y_t-0', 'y_t-1', 'y_t-2', 'y_t-3']\n",
        "\n",
        "  if method == 'price':\n",
        "    for field in x_fields:\n",
        "      norm_df[field + '_norm'] = df[field] \n",
        "      \n",
        "    for field in y_fields:\n",
        "      norm_df[field + '_norm'] = df[field] \n",
        "    \n",
        "  if method == 'points': \n",
        "    for field in x_fields:\n",
        "      if 'volume' in field:\n",
        "        norm_df[field + '_norm'] = df[field] / 100\n",
        "      elif 'audusd' in field:\n",
        "        norm_df[field + '_norm'] = (df[field] - df['x_audusd_open']) * 100000 \n",
        "      elif 'eurusd' in field:\n",
        "        norm_df[field + '_norm'] = (df[field] - df['x_eurusd_open']) * 100000\n",
        "\n",
        "    for field in y_fields:\n",
        "      norm_df[field + '_norm'] = (df[field] -  df['audusd_open']) * 100000 \n",
        "\n",
        "  if method == 'percentage':\n",
        "    for field in x_fields:\n",
        "      if 'volume' in field:\n",
        "        norm_df[field + '_norm'] = df[field] / 10000\n",
        "      elif 'audusd' in field:\n",
        "        norm_df[field + '_norm'] = (df[field] - df['x_audusd_open']) / df[field] * 100 \n",
        "      elif 'eurusd' in field:\n",
        "        norm_df[field + '_norm'] = (df[field] - df['x_eurusd_open']) / df[field] * 100\n",
        "      \n",
        "    for field in y_fields:\n",
        "      norm_df[field + '_norm'] = (df[field] - df['audusd_open']) / df[field] * 100\n",
        "\n",
        "  if method == 'minmax':\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled = scaler.fit_transform(df[x_fields + y_fields])\n",
        "    norm_field_names = [x + '_norm' for x in x_fields + y_fields]\n",
        "    norm_df[norm_field_names] = scaled\n",
        "\n",
        "  if method == 'stddev':\n",
        "    scaler = StandardScaler()\n",
        "    scaled = scaler.fit_transform(df[x_fields + y_fields])\n",
        "    norm_field_names = [x + '_norm' for x in x_fields + y_fields]\n",
        "    norm_df[norm_field_names] = scaled\n",
        "\n",
        "  x_feature_names_norm = [x + '_norm' for x in x_fields]\n",
        "  return norm_df, x_feature_names_norm"
      ],
      "metadata": {
        "id": "-XuKCNtsORbh"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_val(df, x_feature_names_norm):\n",
        "  #\n",
        "  # Create Train and Val datasets \n",
        "  # \n",
        "\n",
        "  x = df[x_feature_names_norm] \n",
        "  y = df['y']\n",
        "  y_points = df['y_change_points']\n",
        "\n",
        "  # Note Fx \"follows\" (time series) so randomization is NOT a good idea\n",
        "  # create train and val datasets. \n",
        "  no_train_samples = int(len(x) * 0.7)\n",
        "  x_train = x[4:no_train_samples]\n",
        "  y_train = y[4:no_train_samples]\n",
        "\n",
        "  x_val = x[no_train_samples:-3]\n",
        "  y_val = y[no_train_samples:-3]\n",
        "  y_val_change_points = y_points[no_train_samples:-3]\n",
        "\n",
        "  return x_train, y_train, x_val, y_val, y_val_change_points"
      ],
      "metadata": {
        "id": "XTyGVvI1Wk1g"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class_weights(y_train, display=True):\n",
        "  \n",
        "  #\n",
        "  # Create class weights \n",
        "  #\n",
        "  from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "  num_ones = np.sum(y_train)\n",
        "  num_zeros = len(y_train) - num_ones \n",
        "  \n",
        "  classes = np.unique(y_train)\n",
        "  class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
        "  class_weights = dict(zip(classes, class_weights))\n",
        "\n",
        "  if display:\n",
        "    print('In the training set we have 0s {} ({:.2f}%), 1s {} ({:.2f}%)'.format(num_zeros, num_zeros/len(y_train)*100, num_ones, num_ones/len(y_train)*100))\n",
        "    print('class weights {}'.format(class_weights))\n",
        "\n",
        "  return class_weights"
      ],
      "metadata": {
        "id": "vrtNA-_QXbv8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss, confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "def show_metrics(lr, x, y_true, y_change_points, display=True):\n",
        "  \n",
        "  # predict from teh val set meas we have predictions and true values as binaries  \n",
        "  y_pred = lr.predict(x)\n",
        "\n",
        "  #basic error types \n",
        "  log_loss_error = log_loss(y_true, y_pred)\n",
        "  score = lr.score(x, y_true)\n",
        "  \n",
        "  #\n",
        "  # Customized metrics  \n",
        "  #\n",
        "  tp = np.where((y_pred == 1) & (y_change_points >= 0), 1, 0).sum()\n",
        "  fp = np.where((y_pred == 1) & (y_change_points < 0), 1, 0).sum()\n",
        "  tn = np.where((y_pred == 0) & (y_change_points < 0), 1, 0).sum()\n",
        "  fn = np.where((y_pred == 0) & (y_change_points >= 0), 1, 0).sum()\n",
        "\n",
        "  precision = 0\n",
        "  if (tp + fp) > 0:\n",
        "    precision = tp / (tp + fp)\n",
        "\n",
        "  recall = 0\n",
        "  if (tp + fn) > 0:\n",
        "    recall = tp / (tp + fn)\n",
        "\n",
        "  f1 = 0\n",
        "  if (precision + recall) > 0:\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "  # output the errors \n",
        "  if display:\n",
        "    print('Errors Loss: {:.4f}'.format(log_loss_error))\n",
        "    print('Errors Score: {:.2f}%'.format(score*100))\n",
        "    print('Errors tp: {} ({:.2f}%)'.format(tp, tp/len(y_val)*100))\n",
        "    print('Errors fp: {} ({:.2f}%)'.format(fp, fp/len(y_val)*100))\n",
        "    print('Errors tn: {} ({:.2f}%)'.format(tn, tn/len(y_val)*100))\n",
        "    print('Errors fn: {} ({:.2f}%)'.format(fn, fn/len(y_val)*100))\n",
        "    print('Errors Precision: {:.2f}%'.format(precision*100))\n",
        "    print('Errors Recall: {:.2f}%'.format(recall*100))\n",
        "    print('Errors F1: {:.2f}'.format(f1*100))\n",
        "\n",
        "  errors = {\n",
        "      'loss': log_loss_error,\n",
        "      'score': score, \n",
        "      'tp': tp,\n",
        "      'fp': fp,\n",
        "      'tn': tn,\n",
        "      'fn': fn,\n",
        "      'precision': precision,\n",
        "      'recall': recall,\n",
        "      'f1': f1\n",
        "      }\n",
        "\n",
        "  return errors"
      ],
      "metadata": {
        "id": "6WkzYiTFYVyO"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for norm_method in ['price', 'points', 'percentage', 'minmax', 'stddev']:\n",
        "  df = load_data()\n",
        "\n",
        "  feature_names =['audusd_open', 'audusd_close', 'audusd_high', 'audusd_low', 'audusd_volume', \\\n",
        "              'eurusd_open', 'eurusd_close', 'eurusd_high', 'eurusd_low', 'eurusd_volume']\n",
        "  df, x_feature_names = create_x_values(df, feature_names)\n",
        "\n",
        "  norm_df, x_feature_names_norm = normalize_data(df, x_feature_names, method=norm_method)\n",
        "  x_train, y_train, x_val, y_val, y_val_change_points = get_train_val(norm_df, x_feature_names_norm)\n",
        "  class_weights = get_class_weights(y_train, display=False)\n",
        "  \n",
        "  lr = LogisticRegression(class_weight=class_weights)\n",
        "  lr.fit(x_train, y_train)\n",
        "\n",
        "  print('Errrors for method {}'.format(norm_method))\n",
        "  errors = show_metrics(lr, x_val, y_val, y_val_change_points, display=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtnSw-VFNinr",
        "outputId": "6a428765-a1ea-4cf8-8652-11ffb277fe01"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Errrors for method price\n",
            "Errors Loss: 12.1699\n",
            "Errors Score: 66.24%\n",
            "Errors tp: 2453 (16.18%)\n",
            "Errors fp: 2422 (15.98%)\n",
            "Errors tn: 4984 (32.88%)\n",
            "Errors fn: 5299 (34.96%)\n",
            "Errors Precision: 50.32%\n",
            "Errors Recall: 31.64%\n",
            "Errors F1: 38.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Errrors for method points\n",
            "Errors Loss: 28.0255\n",
            "Errors Score: 22.25%\n",
            "Errors tp: 6799 (44.85%)\n",
            "Errors fp: 6426 (42.39%)\n",
            "Errors tn: 980 (6.47%)\n",
            "Errors fn: 953 (6.29%)\n",
            "Errors Precision: 51.41%\n",
            "Errors Recall: 87.71%\n",
            "Errors F1: 64.82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Errrors for method percentage\n",
            "Errors Loss: 14.1055\n",
            "Errors Score: 60.87%\n",
            "Errors tp: 3000 (19.79%)\n",
            "Errors fp: 2983 (19.68%)\n",
            "Errors tn: 4423 (29.18%)\n",
            "Errors fn: 4752 (31.35%)\n",
            "Errors Precision: 50.14%\n",
            "Errors Recall: 38.70%\n",
            "Errors F1: 43.68\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Errrors for method minmax\n",
            "Errors Loss: 12.9094\n",
            "Errors Score: 64.18%\n",
            "Errors tp: 2712 (17.89%)\n",
            "Errors fp: 2658 (17.54%)\n",
            "Errors tn: 4748 (31.32%)\n",
            "Errors fn: 5040 (33.25%)\n",
            "Errors Precision: 50.50%\n",
            "Errors Recall: 34.98%\n",
            "Errors F1: 41.34\n",
            "Errrors for method stddev\n",
            "Errors Loss: 13.7131\n",
            "Errors Score: 61.95%\n",
            "Errors tp: 2959 (19.52%)\n",
            "Errors fp: 2905 (19.16%)\n",
            "Errors tn: 4501 (29.69%)\n",
            "Errors fn: 4793 (31.62%)\n",
            "Errors Precision: 50.46%\n",
            "Errors Recall: 38.17%\n",
            "Errors F1: 43.46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    }
  ]
}